\chapter{Ray Tracing}
Die Idee beim \introduce{Ray Tracing} ist es, für jeden Pixel, alle Objekte zu finden, die diesen Pixel beeinflussen.
Anhand all dieser Objekte wird die Pixelfarbe bestimmt.
Dazu wird vom Rückwärtslichttransport ausgegangen.
Man startet an der Kamera und sucht alle Pfade, auf denen das Licht dort hin gelangt.
Dabei wird angenommen, dass der Lichttransport den Gesetzen der geometrischen Optik folgt.

\section{Abtastung}
Ein \introduce{Rasterbild} ist eine äquidistante Abtastung eines Bildsignals.
Das Bildsignal wird also vereinfachend als stückweise konstante Funktion aufgefasst.
Die bringt Probleme wie \introduce{Aliasing} oder den \introduce{Moiré-Effekt} mit sich.

\begin{Theorem}[\textsc{Nyquist}-\textsc{Shannon}-Abtasttheorem]
	Ein kontinuierliches, bandbegrenztes Signal mit einer maximalen Frequenz $f_{max}$ muss mit einer Frequenz echt größer als $2 f_{max}$ abgetastet werden, damit aus dem diskreten Signal das Ursprungssignal exakt rekostruiert werden kann.
\end{Theorem}

Ist die Abtastfrequenz zu gering, kommt es zu Aliasing.
Ein möglicher Lösungsansatz ist eine Vorfilterung des Signals, bei der hohe Frequenzen entfernt werden.
Dies ist jedoch im allgemeinen Fall nicht möglich.
Eine andere Möglichkeit ist eine \introduce{Überabtastung} mit anschließender Filterung.

\section{Lochkamera}
Am einfachsten zur Bildsynthese ist das Modell der Lochkamera.
Sie ist definiert durch die Position ihrer Öffnung und der Bildebene.
Da keine Linse verwendet wird, hat sie unbeschränkte Schärfentiefe.

Eine \introduce{virtuelle Kamera} ist definiert durch ihre Position und Blickrichtung, sowie die Orientierung der Vertikalen Achse.
Dazu die Breite und Höhe der Bildebene und ihr Abstand \emph{vor} der Kamera.

Bei der Bildsynthese kann \introduce{objektbasiert} oder \introduce{bildbasiert} vorgegangen werden.

Beim objektbasierten Rendern werden für jedes Objekt alle Pixel bestimmt, die es überdeckt.
Dann wird die Farbe dieser Pixel ermittelt.

Beim bildbasierten Rendern werden für jeden Pixel alle an dieser Stelle sichtbaren Objekte bestimmt.
Daraus wird die Pixelfarbe ermittelt.

\section{Ray Tracing}
Ray Tracing besteht aus drei Schritten, die in dieser Reihenfolge ausgeführt werden.
\begin{enumerate}
	\item \introduce{Ray Generation} Für jeden Pixel wird ein Strahl von der Kamera durch diesen Pixel erzeugt.
	\item \introduce{Ray Intersection} Für jeden Strahl wird das Objekt gefunden, das die Kamera an diesem Pixel sieht.
	Es ist das Objekt, das diesen Strahl schneidet und dessen Schnittpunkt am nächsten an der Kamera liegt.
	\item \introduce{Beleuchtungsberechnung} Farbe und Schattierung dieses Objekts an dieser Stelle wird berechnet.
	Dazu können rekursiv weitere Strahlen erzeugt werden, um \zB reflektierende Oberflächen darzustellen.
\end{enumerate}

\section{Ray Generation}
Die virtuelle Kamera ist definiert durch ihr Projektionszentrum $e$ (engl. eye) und einen $up$-Vektor mit $\norm{up} = 1$.
Sei $z$ der Zielpunkt eines Strahls.
Definiere dann
\[
	w = \frac{(e - z)}{\norm{e - z}} \text{,} \qquad
	u = up \times w \text {,} \qquad
	v = w \times u \text{.}
\]
Dabei ist $w$ die negative Blickrichtung.

Die Bildebene ist gegeben durch ihren Abstand $d$ zur Kamera, ihren linken und rechten Rand $l$ und $r$ sowie ihren oberen und unteren Rand $b$ und $t$.
Strahlen von $e$ aus zu einem Punkt $s$ auf der Bildebene sind nun beschrieben durch:
\[
	s = \lambda_1 u + \lambda_2 v - dw \qquad \lambda_1 \in [l, r] \quad \lambda_2 \in [b, t] 
\]
Typischerweise ist das Sichtfeld symmetrisch, es gilt also $l = -r$ und $t = -b$.
Das Verhältnis aus der Breite zur Höhe des Bildschirms heißt \introduce{Aspect Ratio}.

\section{Ray Intersection}
Geometrische Objekte können auf drei verschiedene Arten beschrieben werden:
\begin{itemize}
	\item \introduce{Parameterdarstellung}
	Einsetzen aller gültigen Parameterwerte liefert alle Punkte des Objekts.
	\item \introduce{Explizite Darstellung}
	Es ist eine Funktion gegeben, die an jeder Position beschreibt, ob das Objekt an dieser Position ist.
	\item \introduce{Implizite Darstellung}
	Alle Punkte des Objekts bilden die Lösungsmenge eines Systems von Gleichungen.
\end{itemize}

\subsection{Kugelschnitt}
Alle Punkte auf der Kugeloberfläche $K$ haben Abstand $r$ vom Mittelpunkt $c = (c_x, c_y, c_z)$.
Die implizite Darstellung der Kugel ist
\[
	K = \{(x, y, z) \mid \norm{(x - c_x, y - c_y, z - c_z)} = r\} \text{.}
\]
Sei $r(t) = e + td$ ein mit $t \in \mathbb{R}_+$ parametriesierter Strahl.
Für den Schnittpunkt aus Kugel und Strahl ergibt sich:
\begin{align*}
	0 &= \norm{r(t) - c}^2 - r^2 \\
		&= \norm{e + td - c}^2 - r^2 \\
		&= \dotproduct{e + td - c}{e + td - c} - r^2 \\
		&= \underbrace{\dotproduct{e - c}{e - c} - r^2}_c + \underbrace{2 \dotproduct{td}{e - c}}_{b \cdot t} + \underbrace{t^2 \dotproduct{d}{d}}_{a \cdot t^2}
\end{align*}
Mit der Mitternachtsformel
\[
	t_{1,2} = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}
\]
lassen sich nun die Parameter $t_1$ und $t_2$ bestimmen.
\[
	D = b^2 - 4ac
\]
heißt ist die Diskriminante.
Ist $D < 0$, gibt es keinen Schnittpunkt.
Ist sie gleich $0$, gibt es genau einen Schnittpunkt bei $r(t_1) = r(t_2)$.
Ist $D$ positiv, gibt es bei $r(t_1)$ und $r(t_2)$ jeweils einen Schnittpunkt.
Die Parameter $t_1$ und $t_2$ können kleiner als $0$ sein.
In diesem Fall liegt der Schnittpunkt hinter der Kamera und sollte nicht betrachtet werden.

\subsection{Ebenenschnitt}
Eine Ebene im $\mathbb{R}^3$ hat die implizite Darstellung
\[
	E = \{(x, y, z) \mid ax + by + cz + d = 0, \quad
	a, b, c, d \in \mathbb{R}, \quad
	a, b, c, \neq 0\} \text{.}
\]
Mit zwei nicht kollinearen Vektoren in der Ebene lässt sich die Normale $n$ berechnen.
Sei $r(t) = e + td$ ein Strahl mit $\norm{d} = 1$.
Dazu sei $\dotproduct{x}{n} - d = 0$ mit $\norm{n}$ die Ebene in Hesse-Normalform.
\begin{align*}
	0 &= \dotproduct{e + td}{n} - d \\
	  &= \dotproduct{e}{n} + t \dotproduct{d}{n} - d
\end{align*}
Damit folgt für den Parameter $t$:
\[
	t = \frac{d - \dotproduct{e}{n}}{\dotproduct{d}{n}}
\]
Fall $\dotproduct{d}{n} = 0$ gilt, sind Strahl und Ebene parallel und es exitiert kein Schnittpunkt.
Andernfalls schneiden sich Strahl und Ebene im Punkt $r(t)$.
Wenn $t < 0$ ist, liegt der Schnittpunkt hinter der Kamera und sollte ignoriert werden.

\subsection{Dreiecksschnitt}
Um einen Schintt zwischen einem Strahl und einem Dreieck zu berechnen, muss zuerst ein Schnittpunkt des Strahl mit der vom Dreieck aufgespannten Ebene gefunden werden.
Die Koordianten des Schnittpunktes können dann in baryzentrische Koordinaten überführt und auf Positivität überprüft werden.

\section{Beleuchtungsberechnung}
Beleuchtung ist essentiell für einen dreidimensionalen Eindruck.
Ein wichtiger Teil der Beleuchtungsberechungen ist die \introduce{Reflexion}.
Es gibt zwei Extreme.
Bei der \introduce{spekularen Reflexion} wird das Licht nur anhand eines Strahls reflektiert, wobei Einfallswinkel gleich Ausfallswinkel gilt.
Dagegen wird das Licht bei der \introduce{diffusen/lambterschen Reflexion} zu gleichen Teilen in alle Richtungen gestreut.

Im Folgenden wird nur Reflexion an der Oberfläche von Objekten behandelt.

\subsection{Bidirectional Reflectance Distribution Function - BRDF}
Eine \introduce{BRDF (Bidirectional Reflectance Distribution Function)} ist ein radiometrisches Konzept, um die Reflexion an einem Oberflächenpunkt zu beschreiben.
Sie gibt das Verhältnis von ausgehendem zu einfallendem Licht an einem Oberflächenpunkt an.
Um Materialien abzubilden, muss die BRDF erst aufwendig an diesem Material gemessen werden.

\subsection{Phong-Beleuchtungsmodell}
Das \introduce{Phong-Beleuchtungsmodell} ist ein phänomenologisches (also physikalische nicht korrektes) Modell zur Darstellung der Reflexion, anhand von drei Komponenten, die von den Materialparametern $k_a$, $k_d$ und $k_s$ sowie dem Phong-Exponenten $n$ abhängen:
\begin{itemize}
	\item \introduce{Ambient}
	Die indirekte Beleuchtung, also Licht, das von anderen Oberflächen reflektiert wird.
	Es ergibt sich der Anteil $I_a = k_a \cdot I_L$.
	\item \introduce{Diffus}
	Der Anteil der lambertschen Reflexion.
	Für den diffusen Anteil ergibt sich $I_d = k_d \cdot I_L \cdot \cos \theta = k_d \cdot I_L \cdot \dotproduct{N}{L}$.
	Dabei ist $I_L$ die Intensität der Lichtquelle, $N$ die normierte Normale am Oberflächenpunkt und $L$ der normierte Vektor zur Lichtquelle.
	\item \introduce{Spekular}
	Spekulare Reflexion bzw. perfekte Spiegelung.
	Die spekulare Reflexion findet ausschließlich in Richtung $R_L$ statt.
	Der Vektor $R_L$ ist die Spiegelung des Vektors $L$ zur Lichtquelle an der Oberflächennormalen $N$.
	Sind alle Vektoren normiert, ergibt sich $R_L = 2N \cdot \dotproduct{N}{L} - L$.

	Durch gerichtete Reflexion entstehen Glanzlichter.
	Die Stärke der Spiegelung fällt für von $R_L$ verschiedene Richtungen stark ab.
	Der Abfall wird durch $\cos^n \alpha$ modelliert.
	Der spekulare Anteil ergibt sich damit zu $I_s = k_s \cdot I_L \cdot \cos^n \alpha = k_s \cdot I_L \cdot \dotproduct{R_L}{V}^n$.
\end{itemize}
Die Gesamtbeleuchtung ergibt sich durch
\begin{align*}
	I &= I_a + I_d + I_s \\
	  &= k_a \cdot I_L +
	  	 k_d \cdot I_L \cdot \dotproduct{N}{L} +
	  	 k_s \cdot I_L \cdot \dotproduct{R_L}{V}^n \text{.}
	\end{align*}
Die Reflexionskoeffizienten $k_a$, $k_d$ und $k_s$ sind theoretisch Wellenlängenabhängig und werden deshalb oft für drei Wellenlängen (rot, grün, blau) angegeben.

Diffuse Reflexionen haben meist die Farbe der Oberfläche.
Spekulare Reflexionen haben meist die Farbe der Oberfläche, wenn es sich um Metalle handelt.
Ansonsten oft die Farbe der Lichtquelle.

Bei der Berechnung der Beleuchtungen ist man nur an den Richtungen interessiert, für die die Skalarprodukte positiv sind.

Optional kann das Phong-Beleuchtungsmodell um einen Emmisionsterm ersetzt werden.

\subsection{Berechnung der Normalen}
